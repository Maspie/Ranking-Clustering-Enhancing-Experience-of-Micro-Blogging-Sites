{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maspie/Ranking-Clustering-Enhancing-Experience-of-Micro-Blogging-Sites/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJFXSu41KQSG",
        "outputId": "44761e7f-3273-4649-e4e8-ec644831b4d8"
      },
      "id": "FJFXSu41KQSG",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmMOHL4GKYO1",
        "outputId": "1508fe84-348a-4b0b-8ede-2eeb0f232519"
      },
      "id": "fmMOHL4GKYO1",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.17)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.36)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.50)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.53)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.48->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting saved Embeddings and cleaned tweet dataset"
      ],
      "metadata": {
        "id": "HP923yigQTr3"
      },
      "id": "HP923yigQTr3"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to download a file from Google Drive\n",
        "def download_file_from_google_drive(url, local_dest):\n",
        "    response = requests.get(url)\n",
        "    with open(local_dest, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "npy_url = 'https://drive.google.com/uc?id=1AeAVk_UX9drv3yt2ooGOq5ODed2YlQgZ&export=download'\n",
        "csv_url = 'https://drive.google.com/uc?id=1yh9s5SJ5SgEdoVbA_6OjCOFwUs5AQySX&export=download'\n",
        "\n",
        "# Download and load embeddings\n",
        "npy_local_path = 'temp_file.npy'\n",
        "download_file_from_google_drive(npy_url, npy_local_path)\n",
        "loaded_embeddings = np.load(npy_local_path)\n",
        "\n",
        "# Download and load CSV data\n",
        "data = pd.read_csv(csv_url)\n",
        "data['embeddings'] = list(loaded_embeddings)\n"
      ],
      "metadata": {
        "id": "WZcKSHCrFB8i"
      },
      "id": "WZcKSHCrFB8i",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation and Ranking of tweets"
      ],
      "metadata": {
        "id": "vsJgEO_CQ2k4"
      },
      "id": "vsJgEO_CQ2k4"
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_tweets(cluster_data, selected_tweet_embedding, selected_tweet_index):\n",
        "    \"\"\"Recommend the top 5 similar tweets based on distance, excluding the selected tweet.\"\"\"\n",
        "    cluster_data = cluster_data.copy()\n",
        "    # Calculate the distance for each tweet in the cluster\n",
        "    cluster_data['distance'] = cluster_data['embeddings'].apply(lambda emb: np.linalg.norm(emb - selected_tweet_embedding))\n",
        "    # Exclude the currently selected tweet from the recommendations\n",
        "    filtered_data = cluster_data[cluster_data.index != selected_tweet_index]\n",
        "    # Return the top 5 tweets with the smallest distance, excluding the current tweet\n",
        "    return filtered_data.nsmallest(5, 'distance')\n",
        "\n"
      ],
      "metadata": {
        "id": "wfGsXVGmGPo_"
      },
      "id": "wfGsXVGmGPo_",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Retriever"
      ],
      "metadata": {
        "id": "LUhgK-YURCMc"
      },
      "id": "LUhgK-YURCMc"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def retrieve_similar_tweets(cluster_data, selected_tweet_embedding, threshold=0.8):\n",
        "    \"\"\"Retrieve tweets with at least 80% similarity to the selected tweet based on embeddings.\n",
        "\n",
        "    Args:\n",
        "    cluster_data (DataFrame): DataFrame containing tweets and their embeddings.\n",
        "    selected_tweet_embedding (np.array): Embedding of the selected tweet.\n",
        "    threshold (float): The similarity threshold.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: Filtered tweets with at least the specified similarity.\n",
        "    \"\"\"\n",
        "    # Calculate cosine similarity between the selected tweet embedding and all embeddings in the cluster\n",
        "    similarities = cosine_similarity([selected_tweet_embedding], cluster_data['embeddings'].tolist())[0]\n",
        "\n",
        "    # Filter tweets based on the similarity threshold\n",
        "    return cluster_data[similarities >= threshold]\n"
      ],
      "metadata": {
        "id": "htUi_RWzGdI3"
      },
      "id": "htUi_RWzGdI3",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get main words from cluster/tweets"
      ],
      "metadata": {
        "id": "-hx15venVdSg"
      },
      "id": "-hx15venVdSg"
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "def create_word_cloud_string(tweets):\n",
        "    \"\"\"Create a single string from the most common words in the tweets using a word cloud.\n",
        "\n",
        "    Args:\n",
        "    tweets (Series): Series containing the text of the tweets.\n",
        "\n",
        "    Returns:\n",
        "    str: A single string representing the most common words.\n",
        "    \"\"\"\n",
        "    # Combine all tweets into one large text\n",
        "    combined_text = ' '.join(tweets)\n",
        "\n",
        "    # Generate word cloud\n",
        "    wordcloud = WordCloud(width=800, height=400).generate(combined_text)\n",
        "\n",
        "    # Extract words from the word cloud\n",
        "    words = list(wordcloud.words_.keys())\n",
        "\n",
        "    # Return a single string composed of these words\n",
        "    return ' '.join(words)\n"
      ],
      "metadata": {
        "id": "8okfzNZJKHtm"
      },
      "id": "8okfzNZJKHtm",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "open_api_key= \"sk-proj-IMaWGJ7t6P0rV6b5n9jBT3BlbkFJ7XnLJQX45rp9ZGGIoQhz\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = open_api_key"
      ],
      "metadata": {
        "id": "4pqMqa53KcjK"
      },
      "id": "4pqMqa53KcjK",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Generation"
      ],
      "metadata": {
        "id": "y-vC6IdoVnrE"
      },
      "id": "y-vC6IdoVnrE"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "def generate_summary(text):\n",
        "    \"\"\"Generate a summary using OpenAI's API\n",
        "\n",
        "    Args:\n",
        "    text (str): Text to summarize.\n",
        "\n",
        "    Returns:\n",
        "    str: Generated summary.\n",
        "    \"\"\"\n",
        "    chat_messages = [\n",
        "        SystemMessage(content='You are an expert assistant with expertise in summarizing what different words say'),\n",
        "        HumanMessage(content=f'Please provide a short and concise summary of the following text:\\n TEXT: {text}')\n",
        "    ]\n",
        "\n",
        "    llm = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
        "    return llm(chat_messages).content\n"
      ],
      "metadata": {
        "id": "IIpyol3hKKRl"
      },
      "id": "IIpyol3hKKRl",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterative Kmeans, Ranking, Recommendation, Retrieval Augmented Generation"
      ],
      "metadata": {
        "id": "mYLVlfiRVvZj"
      },
      "id": "mYLVlfiRVvZj"
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def main_process(data, max_depth=3):\n",
        "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "    data['cluster'] = kmeans.fit_predict(list(data['embeddings']))\n",
        "\n",
        "    initial_index = np.random.choice(data.index)\n",
        "    selected_tweet = data.loc[initial_index]\n",
        "    process_tweet_selection(data, selected_tweet, 0, max_depth)\n",
        "\n",
        "def print_summary(summary):\n",
        "    \"\"\"Print the summary in a formatted way to enhance readability.\"\"\"\n",
        "    wrapped_text = textwrap.fill(summary, width=100)\n",
        "    print(\"\\nCluster Summary:\")\n",
        "    print(wrapped_text)\n",
        "\n",
        "\n",
        "def process_tweet_selection(data, selected_tweet, depth, max_depth):\n",
        "    print(\"\\nSelected Tweet:\", selected_tweet['text'])\n",
        "    cluster_data = data[data['cluster'] == selected_tweet['cluster']].copy()\n",
        "    similar_tweets = retrieve_similar_tweets(cluster_data, selected_tweet['embeddings'])\n",
        "    word_cloud_string = create_word_cloud_string(similar_tweets['text'])\n",
        "    cluster_summary = generate_summary(word_cloud_string)\n",
        "    print_summary(cluster_summary)\n",
        "\n",
        "    recommended_tweets = recommend_tweets(similar_tweets, selected_tweet['embeddings'], selected_tweet.name)\n",
        "    print(\"\\nTop 5 Recommended Tweets:\")\n",
        "    for i, tweet in enumerate(recommended_tweets['text'], 1):\n",
        "        print(f\"{i}. {tweet}\")\n",
        "\n",
        "    if len(cluster_data) > 30 and depth < max_depth:\n",
        "        selected_option = int(input(\"Select a tweet (1-5): \")) - 1\n",
        "        new_selected_tweet_index = recommended_tweets.iloc[selected_option].name\n",
        "        new_selected_tweet = data.loc[new_selected_tweet_index]\n",
        "        process_tweet_selection(data, new_selected_tweet, depth + 1, max_depth)\n",
        "    else:\n",
        "        print(\"\\nProcess finished, fewer than 30 tweets left in the cluster or maximum depth reached.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "main_process(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywt_x8-4KMvJ",
        "outputId": "6d92b278-995c-4f91-ba82-0bb3ab0868a6"
      },
      "id": "ywt_x8-4KMvJ",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected Tweet: Musician Kalle Mattson Recreates 34 Classic Album Covers in Clever Music Video for Û÷AvalancheÛª http://t.co/VBSwhz4s2V\n",
            "\n",
            "Cluster Summary:\n",
            "The text mentions a music video by musician Kalle Mattson that recreates a classic album cover, a\n",
            "NHL hockey shirt, a blog post about a little piece of music, and various music references including\n",
            "artists like BTS, Erykah Badu, and Smashing Pumpkins. There are also mentions of a video game and a\n",
            "podcast episode called \"The Piano Entertainer Ep – Collide.\"\n",
            "\n",
            "Top 5 Recommended Tweets:\n",
            "1. Musician Kalle Mattson Recreates 34 Classic Album Covers in Clever Music Video for 'Avalanche' http://t.co/yDJpOpH1DW\n",
            "2. I liked a @YouTube video http://t.co/TNXQuOr1wb Kalle Mattson - 'Avalanche' (Official Video)\n",
            "3. What a feat! Watch the #BTS of @kallemattson's incredible music video for #Avalanche: https://t.co/3W6seA9tuv ????\n",
            "4. Great one time deal on all Avalanche music and with purchase get a Neal Rigga shirt http://t.co/4VIRXkgMpC\n",
            "5. Avalanche City - Sunset http://t.co/48h3tLvLXr #nowplay #listen #radio\n",
            "Select a tweet (1-5): 4\n",
            "\n",
            "Selected Tweet: Great one time deal on all Avalanche music and with purchase get a Neal Rigga shirt http://t.co/4VIRXkgMpC\n",
            "\n",
            "Cluster Summary:\n",
            "The text mentions a musician, Kalle Mattson, recreating classic album covers. It also includes\n",
            "references to sports, a blog post, and an eBay listing for a Chevrolet truck. The text encourages\n",
            "viewers to favorite, share, and purchase.\n",
            "\n",
            "Top 5 Recommended Tweets:\n",
            "1. #Colorado #Avalanche Men's Official Colorado Avalanche Reebok T-Shirt XL Blue 100% Cotton http://t.co/ZNSvsTGwx3 #NHL #Hockey\n",
            "2. Musician Kalle Mattson Recreates 34 Classic Album Covers in Clever Music Video for 'Avalanche' http://t.co/yDJpOpH1DW\n",
            "3. Musician Kalle Mattson Recreates 34 Classic Album Covers in Clever Music Video for Û÷AvalancheÛª http://t.co/VBSwhz4s2V\n",
            "4. Avalanche City - Sunset http://t.co/48h3tLvLXr #nowplay #listen #radio\n",
            "5. Free Ebay Sniping RT? http://t.co/RqIPGQslT6 Chevrolet : Avalanche Ltz Lifted 4x4 Truck ?Please Favorite &amp; Share\n",
            "Select a tweet (1-5): 1\n",
            "\n",
            "Selected Tweet: #Colorado #Avalanche Men's Official Colorado Avalanche Reebok T-Shirt XL Blue 100% Cotton http://t.co/ZNSvsTGwx3 #NHL #Hockey\n",
            "\n",
            "Cluster Summary:\n",
            "The text includes mentions of an official shirt for a music video, a musician recreating classic\n",
            "album covers, appreciation for a blog post, and various sports-related topics like skiing and NHL\n",
            "hockey. There are also references to a Chevrolet lifted truck for sale and a music video by Kalle\n",
            "Mattson.\n",
            "\n",
            "Top 5 Recommended Tweets:\n",
            "1. #WeLoveLA #NHLDucks Avalanche Defense: How They Match vs St. Louis Blues http://t.co/9v1RVCOMH2 #SportsRoadhouse\n",
            "2. Great one time deal on all Avalanche music and with purchase get a Neal Rigga shirt http://t.co/4VIRXkgMpC\n",
            "3. 1-6 TIX Calgary Flames vs COL Avalanche Preseason 9/29 Scotiabank Saddledome http://t.co/5G8qA6mPxm\n",
            "4. PATRICK ROY 1998-99 UPPER DECK SPX #171 FINITE 1620 MADE COLORADO AVALANCHE MINT http://t.co/uHfM1r3Tq5 http://t.co/QulgaKebHB\n",
            "5. Avalanche City - Sunset http://t.co/48h3tLvLXr #nowplay #listen #radio\n",
            "Select a tweet (1-5): 5\n",
            "\n",
            "Selected Tweet: Avalanche City - Sunset http://t.co/48h3tLvLXr #nowplay #listen #radio\n",
            "\n",
            "Cluster Summary:\n",
            "The text discusses various topics such as music, sports, and personal updates. It mentions a music\n",
            "video, a musician recreating classic album covers, a hockey game, and a wedding. Additionally, there\n",
            "are references to a Chevrolet truck, eBay, and a blog post. The overall theme centers around music\n",
            "and personal experiences.\n",
            "\n",
            "Top 5 Recommended Tweets:\n",
            "1. #Colorado #Avalanche Men's Official Colorado Avalanche Reebok T-Shirt XL Blue 100% Cotton http://t.co/ZNSvsTGwx3 #NHL #Hockey\n",
            "2. A little piece I wrote for the Avalanche Designs blog! I'd appreciate it greatly if you checked it out :-)  https://t.co/rfvjh58eF2\n",
            "3. Beautiful Sweet Avalanche Faith and Akito roses with lots of frothy gyp. http://t.co/RaqUpzFkJY #weddinghour http://t.co/quNxocXCgA\n",
            "4. You are the avalanche. One world away. My make believing. While I'm wide awake.\n",
            "5. PATRICK ROY 1998-99 UPPER DECK SPX #171 FINITE 1620 MADE COLORADO AVALANCHE MINT http://t.co/uHfM1r3Tq5 http://t.co/QulgaKebHB\n",
            "\n",
            "Process finished, fewer than 30 tweets left in the cluster or maximum depth reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qtri4bP-KOa4"
      },
      "id": "qtri4bP-KOa4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}